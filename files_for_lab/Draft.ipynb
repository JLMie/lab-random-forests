{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f0d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "donors_tar = pd.read_csv('target.csv')\n",
    "donors_num = pd.read_csv('categorical.csv')\n",
    "donors_cat = pd.read_csv('numerical.csv')\n",
    "\n",
    "Donors = pd.concat([donors_tar,donors_cat, donors_num], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21891904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET_B</th>\n",
       "      <th>TARGET_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95407</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95408</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95409</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95410</th>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95411</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95412 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TARGET_B  TARGET_D\n",
       "0             0       0.0\n",
       "1             0       0.0\n",
       "2             0       0.0\n",
       "3             0       0.0\n",
       "4             0       0.0\n",
       "...         ...       ...\n",
       "95407         0       0.0\n",
       "95408         0       0.0\n",
       "95409         0       0.0\n",
       "95410         1      18.0\n",
       "95411         0       0.0\n",
       "\n",
       "[95412 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donors_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb9adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Donors.drop(['TARGET_B','TARGET_D'], axis = 1) #boston features\n",
    "y = Donors['TARGET_B'] #Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ab6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5a8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c3c42",
   "metadata": {},
   "source": [
    "#### Apply the Random Forests algorithm but this time only by upscaling the data to deal with the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1283865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63369, 338) (3419, 338)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample # WHY we are just oversampling the train one?\n",
    "\n",
    "no_donate = train_set[train_set['TARGET_B'] == 0] #we are splitting in the majority and the minority \n",
    "yes_donate = train_set[train_set['TARGET_B'] == 1] # this is the minority \n",
    "\n",
    "print(no_donate.shape, yes_donate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bbb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "yes_donate_upsampled = resample(yes_donate, \n",
    "                                    replace=True,\n",
    "                                    n_samples = len(no_donate),\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b16216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63369, 338)\n",
      "(63369, 338)\n"
     ]
    }
   ],
   "source": [
    "print(no_donate.shape)\n",
    "print(yes_donate_upsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10950053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the same numbers of values in both categories. 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb90b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_ups = pd.concat([no_donate, yes_donate_upsampled], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9953af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126738, 338)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_ups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8544fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset_ups = trainset_ups.drop(['TARGET_B'], axis = 1) #boston features\n",
    "y_trainset_ups = trainset_ups['TARGET_B'] #Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c374274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STATE', 'HOMEOWNR', 'GENDER', 'RFA_2R', 'RFA_2A', 'GEOCODE2', 'DOMAIN_A']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorial_heads = X_trainset_ups.select_dtypes(include=['object']).columns.tolist()\n",
    "categorial_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c0c5082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126738, 337)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(126738,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(28624, 337)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(28624,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_trainset_ups.shape)\n",
    "display(y_trainset_ups.shape)\n",
    "display(X_test.shape) # We oversampled just in train set, why?\n",
    "display(y_test.shape) # We oversampled just in train set, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a8a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee6b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset_ups_num = X_trainset_ups.select_dtypes(np.number)\n",
    "X_trainset_ups_cat = X_trainset_ups.select_dtypes(object) \n",
    "### Why Should not create a test_set? test_set = X_test y_test\n",
    "## Because we are not going to feed the model with that, right?\n",
    "# The model will be trained with an upbanlance and proccesed(encode and scaled). WHY? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57f7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test.select_dtypes(np.number) \n",
    "X_test_cat = X_test.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2bd5e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCODE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>WEALTH1</th>\n",
       "      <th>HIT</th>\n",
       "      <th>MALEMILI</th>\n",
       "      <th>MALEVET</th>\n",
       "      <th>VIETVETS</th>\n",
       "      <th>WWIIVETS</th>\n",
       "      <th>LOCALGOV</th>\n",
       "      <th>...</th>\n",
       "      <th>DOB_YR</th>\n",
       "      <th>DOB_MM</th>\n",
       "      <th>MINRDATE_YR</th>\n",
       "      <th>MINRDATE_MM</th>\n",
       "      <th>MAXRDATE_YR</th>\n",
       "      <th>MAXRDATE_MM</th>\n",
       "      <th>LASTDATE_YR</th>\n",
       "      <th>LASTDATE_MM</th>\n",
       "      <th>FIRSTDATE_YR</th>\n",
       "      <th>FIRSTDATE_MM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33476</th>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87878</th>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>28</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>12</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85573</th>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68933</th>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39226</th>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23926</th>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12881</th>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62494</th>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>8</td>\n",
       "      <td>88</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27778</th>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126738 rows Ã— 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TCODE   AGE  INCOME  WEALTH1  HIT  MALEMILI  MALEVET  VIETVETS  \\\n",
       "33476      1  54.0       7        2    4         0       34        23   \n",
       "87878     28  28.0       6        9    0         3       36        39   \n",
       "4784      28  74.0       5        9    0         5       36        32   \n",
       "85573      1  60.0       6        8    0         1       37        40   \n",
       "68933      0  80.0       4        9   12         0       45        17   \n",
       "...      ...   ...     ...      ...  ...       ...      ...       ...   \n",
       "39226      0  75.0       6        6   21         6       41        46   \n",
       "23926      0  60.0       5        9    0         0       23        36   \n",
       "12881      2  62.0       1        9    0         0       37        31   \n",
       "62494      0  71.0       2        9    0         0       19        39   \n",
       "27778      2  52.0       4        8    0         0       25        19   \n",
       "\n",
       "       WWIIVETS  LOCALGOV  ...  DOB_YR  DOB_MM  MINRDATE_YR  MINRDATE_MM  \\\n",
       "33476        49         2  ...      44       1           95           10   \n",
       "87878        18         7  ...      70       1           96            3   \n",
       "4784         33         4  ...      24       1           93           12   \n",
       "85573        22         5  ...      38       1           95            2   \n",
       "68933        63         5  ...      18       1           95            6   \n",
       "...         ...       ...  ...     ...     ...          ...          ...   \n",
       "39226        27        14  ...      23       1           96            2   \n",
       "23926        30         4  ...      38       1           92            5   \n",
       "12881        48         8  ...      36       1           87            5   \n",
       "62494        25         3  ...      27       2           88            9   \n",
       "27778        40         6  ...      46       1           93           10   \n",
       "\n",
       "       MAXRDATE_YR  MAXRDATE_MM  LASTDATE_YR  LASTDATE_MM  FIRSTDATE_YR  \\\n",
       "33476           92           10           95           10            92   \n",
       "87878           96            3           96            3            96   \n",
       "4784            96            3           96            3            87   \n",
       "85573           95           10           95           10            95   \n",
       "68933           95            6           95            6            95   \n",
       "...            ...          ...          ...          ...           ...   \n",
       "39226           96            2           96            2            96   \n",
       "23926           92            3           95            6            86   \n",
       "12881           92            4           96            2            86   \n",
       "62494           94            6           95            8            88   \n",
       "27778           95           10           95           10            93   \n",
       "\n",
       "       FIRSTDATE_MM  \n",
       "33476             1  \n",
       "87878             3  \n",
       "4784              1  \n",
       "85573             2  \n",
       "68933             6  \n",
       "...             ...  \n",
       "39226             2  \n",
       "23926             7  \n",
       "12881             8  \n",
       "62494             9  \n",
       "27778            10  \n",
       "\n",
       "[126738 rows x 330 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainset_ups_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca558a12",
   "metadata": {},
   "source": [
    "#### One Hot Encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25972089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODING Train\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first').fit(X_trainset_ups_cat)\n",
    "encoded_categorical = encoder.transform(X_trainset_ups_cat).toarray()\n",
    "X_trainset_ups_cat_encoded = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out()) # needed to avoid error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e923117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODING TEST\n",
    "encoded_categorical = encoder.transform(X_test_cat).toarray()\n",
    "X_test_cat_encoded = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d965e",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aff042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bde39d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SCALING Train\n",
    "transformer_train = StandardScaler().fit(X_trainset_ups_num)\n",
    "x_standardized = transformer_train.transform(X_trainset_ups_num)\n",
    "\n",
    "X_trainset_ups_num_scaled = pd.DataFrame(x_standardized, columns=X_trainset_ups_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e64579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING Test\n",
    "# Why we don't use here another transfromer. transformer_test = StandardScaler().fit(X_test_num)?\n",
    "x_standardized = transformer_train.transform(X_test_num)\n",
    "\n",
    "X_test_num_scaled = pd.DataFrame(x_standardized, columns=X_test_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3da6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ups_processed = pd.concat([X_trainset_ups_cat_encoded, X_trainset_ups_num_scaled], axis=1)\n",
    "X_test_processed = pd.concat([X_test_cat_encoded, X_test_num_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4544689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ups_processed.reset_index(drop=True, inplace=True)\n",
    "X_test_processed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e33cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainset_ups.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5fe74f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126738, 354)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ups_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdc0955d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28624, 354)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0259eb",
   "metadata": {},
   "source": [
    "### RANDOM FOREST UPBALANCE AND PROCESSED WITHOUT SELECTING FEATURES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc88151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree does not need be feed by scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc6f1f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126738,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainset_ups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8d98c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66788,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b89526b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28624,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8515e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126738, 354)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ups_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44cdcc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28624, 354)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e012cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainSet =  0.6244457068913822\n",
      "TestSet =  0.6038638904415875\n"
     ]
    }
   ],
   "source": [
    "## ValueError: Found input variables with inconsistent numbers of samples: [126738, 66788]\n",
    "# 1124 check_consistent_length(X, y)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20)\n",
    "clf.fit(X_train_ups_processed, y_trainset_ups)\n",
    "print('TrainSet = ',clf.score(X_train_ups_processed, y_trainset_ups))\n",
    "print('TestSet = ',clf.score(X_test_processed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73750aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the score is close to each other, it is working pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95eb9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set_clean_num = X_train_set_clean.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aba91478",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set_clean_cat = X_train_set_clean.select_dtypes(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO I NEED THE TEST SPLIT HERE AGAIN BEFORE SCALE AND ENCODE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21398d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALATION. \n",
    "transformer_2 = StandardScaler().fit(X_train_set_clean_num)\n",
    "x_standardized = transformer_2.transform(X_train_set_clean_num)\n",
    "X_train_set_clean_num_scaled = pd.DataFrame(x_standardized, columns= X_train_set_clean_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55ee7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set_clean_cat.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52345b99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (86786, 31), indices imply (86786, 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder2 \u001b[38;5;241m=\u001b[39m OneHotEncoder(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X_train_set_clean_cat)\n\u001b[0;32m      3\u001b[0m encoded_categorical \u001b[38;5;241m=\u001b[39m encoder2\u001b[38;5;241m.\u001b[39mtransform(X_train_set_clean_cat)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m----> 4\u001b[0m X_train_set_clean_cat_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:695\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    685\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    687\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    693\u001b[0m         )\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    420\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 422\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (86786, 31), indices imply (86786, 24)"
     ]
    }
   ],
   "source": [
    "#ENCODED.\n",
    "encoder2 = OneHotEncoder(drop='first').fit(X_train_set_clean_cat)\n",
    "encoded_categorical = encoder2.transform(X_train_set_clean_cat).toarray()\n",
    "X_train_set_clean_cat_encoded = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "971dc051",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_set_clean_cat_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_processed_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train_set_clean_num_scaled, \u001b[43mX_train_set_clean_cat_encoded\u001b[49m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_set_clean_cat_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_processed_final = pd.concat([X_train_set_clean_num_scaled, X_train_set_clean_cat_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c78e3",
   "metadata": {},
   "source": [
    "#### Use Feature Selections that you have learned in class to decide if you want to use all of the features (Variance Threshold, RFE, PCA, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am goint to use just numerical because in the lesson Jan applied just numericals. \n",
    "# Maybe because in that dataset has just numericals? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36ab6088",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_set_num_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m sel \u001b[38;5;241m=\u001b[39m VarianceThreshold(threshold\u001b[38;5;241m=\u001b[39mvar_threshold) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#### two parameters: threshold and variance_threshold.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m### threshold parameter is the value that will be used to determine whether or not a feature should be included in the model.          \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m## which means that any features with an absolute difference between their mean and standard deviation greater than \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 0.02 (at least 2% of variation among all features) are not included in the model, they will be removed.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sel \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_set_num_scaled\u001b[49m) \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m### Reviewing the lesson. I nor sure if I should to insert just the train or whole numericals scales \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## with a variable that concat. X_train_num_scaled + X_test_num_scaled. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# However after encode categorical they are al well numbers, so....I am confused.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m## Next, we create another variable called temp using sel as its input function and \u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# then transform our training data into a DataFrame object using pd-learn library functions like fit() and transform().\u001b[39;00m\n\u001b[0;32m     21\u001b[0m temp \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mtransform(X_train_set_num_scaled)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_set_num_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_threshold = 0.02\n",
    "\n",
    "# by default var_treshold is 0. But we want to remove the rows are constant and the ones are almost constant too.   \n",
    "# In practise we would scale, the columns first, and then apply threshold, or apply different treshold for different columns.\n",
    "sel = VarianceThreshold(threshold=var_threshold) \n",
    "\n",
    "#### two parameters: threshold and variance_threshold.\n",
    "### threshold parameter is the value that will be used to determine whether or not a feature should be included in the model.          \n",
    "## which means that any features with an absolute difference between their mean and standard deviation greater than \n",
    "# 0.02 (at least 2% of variation among all features) are not included in the model, they will be removed.\n",
    "\n",
    "sel = sel.fit(X_train_set_num_scaled) \n",
    "### Reviewing the lesson. I nor sure if I should to insert just the train or whole numericals scales \n",
    "## with a variable that concat. X_train_num_scaled + X_test_num_scaled. \n",
    "# However after encode categorical they are al well numbers, so....I am confused.\n",
    "\n",
    "## Next, we create another variable called temp using sel as its input function and \n",
    "# then transform our training data into a DataFrame object using pd-learn library functions like fit() and transform().\n",
    "\n",
    "temp = sel.transform(X_train_set_num_scaled)\n",
    "temp = pd.DataFrame(temp)\n",
    "\n",
    "print(X_train_set_num_scaled.shape)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d685362",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temp).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel.variances_ > var_threshold\n",
    "#sel.get_support()\n",
    "#var_list = list(sel.get_support())\n",
    "#len(var_list)\n",
    "##  The code would return a list of all the variances that are less than or equal to the threshold.\n",
    "# I don't understand this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train_set_num_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e72a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(list(zip(X_train_set_num_scaled.columns,var_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_list = [col[0] for col in zip(X_train_set_num_scaled.columns,var_list) if col[1] == False]\n",
    "#len(drop_list)\n",
    "\n",
    "##The code attempts to be used to find a list of columns with support that \n",
    "# are greater than the variable threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WILL DROP MANUALLY THE SAME COLUMNS THAN IN THE ERIN NOTEBOOK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHICH COLULMNS ARE IN MY DATASET?\n",
    "column_names = ['OSOURCE', 'SOLIH', 'VETERANS', 'ZIP', 'Unnamed: 0']\n",
    "\n",
    "missing_columns = []\n",
    "for column in column_names:\n",
    "    if column not in Donors.columns:\n",
    "        missing_columns.append(column)\n",
    "\n",
    "if len(missing_columns) == 0:\n",
    "    print(\"All columns are present in the dataset.\")\n",
    "else:\n",
    "    print(\"The following columns are missing from the dataset:\")\n",
    "    print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecaea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set_num2 = X_train_set_num.drop(drop_list, axis = 1)\n",
    "X_train_processed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed2 = X_test_processed.drop(drop_list, axis = 1)\n",
    "X_test_processed2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb51ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['HVP1','HVP2','HVP3','HVP5','HVP6','HV2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be637bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_all.drop(['HVP1','HVP2','HVP3','HVP5','HVP6','HV2'], axis = 1)\n",
    "X_train.shape\n",
    "X_test = X_test_all.drop(['HVP1','HVP2','HVP3','HVP5','HVP6','HV2'], axis = 1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ac6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba7d529",
   "metadata": {},
   "source": [
    "## Looking at multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train= pd.concat([X_train_processed2, y_train], axis=1)\n",
    "Xy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = Xy_train.corr(method = 'pearson')\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "ax = sns.heatmap(corr_matrix, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a6bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35327c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c784c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07f081df",
   "metadata": {},
   "source": [
    "#### Re-run the Random Forest algorithm to determine if the Feature Selection has improved the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce95d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20)\n",
    "clf.fit(X_train, y_train)\n",
    "print('TrainSet = ',clf.score(X_train, y_train))\n",
    "print('TestSet = ',clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51091c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde0c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, X_test, y_test,cmap=plt.cm.Blues)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ddd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805efd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb68d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9784c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977591a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7215ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ae5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_scaled = X_train_num_scaled.drop(drop_list, axis = 1)\n",
    "X_train_num_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.concat([X_train_processed,y_trainset_ups], axis=1)\n",
    "X2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e9b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340203a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3edb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaedec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd156960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e29977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89c092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6f708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Donors.drop(['TARGET_B','TARGET_D'], axis = 1) #boston features\n",
    "y = Donors['TARGET_B'] #Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc205960",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X.select_dtypes(np.number)\n",
    "X_cat = X.select_dtypes(object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0931ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE HOT ENCODED\n",
    "encoded_categorical = encoder.transform(X_cat).toarray()\n",
    "X_cat_encoded = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f484775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "x_standardized = transformer.transform(X_num)\n",
    "X_num_scaled = pd.DataFrame(x_standardized, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ea7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = pd.concat([X_cat_encoded,X_num_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TrainSet = ',clf.score(X_trans, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_trans) # Here I am using the random forest. \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the random fetures #selecting feature by treshold #rerun random fores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24993c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2460d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ec682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b37ca7",
   "metadata": {},
   "source": [
    "#### Discuss the output and its impact in the business scenario. Is the cost of a false positive equals to the cost of the false negative? How would you change your algorithm or data in order to maximize the return of the business?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45e125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe02d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
